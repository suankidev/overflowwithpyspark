# overflowwithpyspark

  the whole idea behind the repo is to learn spark programming with python

#How evironment_one.yml is created initially

conda env export --name pyspark_dev_env


#how to setup the project
mkdir overflowwithpyspark
cd overflowwithpyspark
conda env create --prefix ./env --file environment_one.yml
conda activate ./env




refrences:

1. https://carpentries-incubator.github.io/introduction-to-conda-for-data-scientists/04-sharing-environments/index.html
2. 


